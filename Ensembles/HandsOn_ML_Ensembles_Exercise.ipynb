{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests\n",
    "\n",
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. If you have trained five different models on the exact same training data and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "- We can definitely get better results if all the models are combined as long as each model is distinct from each other. If there are same type of models in an ensemble then they are most likely to make the same errors hence the overall result might not improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between hard and soft voting classifers?\n",
    "\n",
    "- Hard voting classifiers aggregates the predictions by majority of votes.\n",
    "- While soft voting classifiers considers the prediction probability and picks the class with the highest probability. This gives better results since hihgly confident classes are given more weights. However, not all models support getting prediction probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests or stacking ensembles?\n",
    "\n",
    "- In bagging all predictors are trained on instances with replacement. All predictors can be trained across multiple servers since each predictor is independent of others. Similary for pasting as well as Random Forests.\n",
    "- In boosting methods, each predictor tries to correct the mistakes made by its predecessor. Therefore in this case a next predictor cannot be trained until the previous predictior's predictions has been made. Therefore, they cannot scale and hence cannot be distributed across multiple servers.\n",
    "- Stacking is just like bagging/pasting with an additional *blending* layer(s). Therefore they can be trained across multiple servers as well, but only per layer. Since the next layer can only be trained after the previous layer's predictions has been made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "- During bagging not all training instances get sampled for training. With some math it is found that only 63 % of training instances are sampled on average for each predictor. So those remaining 37 % instances can actually be used as a validation set to check the training performance. This eliminates the need of having a separate validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What makes Extra Trees more random than regular Random Forests? How can this extra randomness help? Are Extra Trees slower or faster than regular random forests?\n",
    "\n",
    "- During the training of Random Forests, only a random subset of features is considerd for splitting. Extra Trees relies on the principle of randomizing the thresholds fopr each feature rather than finding the best possible thresholds. This significantly increases the performance than regular Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    "\n",
    "- If AdaBoost underfits the training data, then more *n_estimators* should be added to the ensemble or regularizing the hyperparameters of the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. If your Gradient Boosting ensemble overfits the training data, should you increase or decrease the learning rate?\n",
    "\n",
    "- The learning rate hyperparameter basically means how much each estimator in the ensemble should contribute. If the gradient boosting is overfitting then it would be ideal to decrease the learning rate. Or techniques like early stopping could be implemented to find the right set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8.\n",
    "\n",
    "#### Load the MNIST data.\n",
    "\n",
    "- 40,000 - Training\n",
    "- 10,000 - Validation\n",
    "- 10,000 - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data,\n",
    "                                                            mnist.target,\n",
    "                                                            test_size=10000,\n",
    "                                                            random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val,\n",
    "                                                  y_train_val,\n",
    "                                                  test_size=10000,\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train various classifiers such as RandomForest, ExtraTrees and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training various classifers\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "et = ExtraTreesClassifier(random_state=42)\n",
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "                     max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                     min_samples_leaf=1, min_samples_split=2,\n",
      "                     min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                     n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                     warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifiers = [rf, et, log_reg]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(\"Training \", classifier)\n",
    "    classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/paperspace/.local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# predictor scores\n",
    "for classifier in classifers:\n",
    "    print(\"Classifier :\", classifier)\n",
    "    print(classifier.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble\n",
    "classifiers = [\n",
    "    ('RandomForest', rf),\n",
    "    ('ExtraTrees', et),\n",
    "    ('LogisticRegression', log_reg)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard voting ensemble\n",
    "hard_voting = VotingClassifier(classifiers,\n",
    "                               voting='hard',\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft voting ensemble\n",
    "soft_voting = VotingClassifier(classifiers,\n",
    "                               voting='soft',\n",
    "                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "voting_ensembles = [hard_voting, soft_voting]\n",
    "\n",
    "for ensemble in voting_ensembles:\n",
    "    print(\"Fitting ensemble : \", ensemble)\n",
    "    ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-ml",
   "language": "python",
   "name": "my-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
